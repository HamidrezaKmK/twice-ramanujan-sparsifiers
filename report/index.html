<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.280">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Twice Ramanujan Sparsifiers</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="index_files/libs/clipboard/clipboard.min.js"></script>
<script src="index_files/libs/quarto-html/quarto.js"></script>
<script src="index_files/libs/quarto-html/popper.min.js"></script>
<script src="index_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="index_files/libs/quarto-html/anchor.min.js"></script>
<link href="index_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="index_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="index_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="index_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="index_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Twice Ramanujan Sparsifiers</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Hamid R. Kamkari, Amandeep Singh </p>
          </div>
  </div>
    
  
    
  </div>
  

</header>

<p>For any graph <span class="math inline">\(G\)</span> a sparsifier <span class="math inline">\(H\)</span> is a graph with far fewer edges that is similar to <span class="math inline">\(G\)</span> in some useful way. While <span class="math inline">\(H\)</span> is much easier to do computation on, it holds the same properties as <span class="math inline">\(G\)</span>, and therefore, it is a reliable way of doing approximate computation on <span class="math inline">\(G\)</span>. For example, if we are dealing with path-finding problems on a dense large graph <span class="math inline">\(G\)</span>, the set of sparsifiers used in <span class="citation" data-cites="chew1989there">(<a href="#ref-chew1989there" role="doc-biblioref">Chew 1989</a>)</span> can be used because they are guaranteed to have almost the same shortest path properties as <span class="math inline">\(G\)</span>.</p>
<p>For illustration, consider the following graph <span class="math inline">\(G\)</span> with four vertices. The new graph obtained has far fewer edges but has the same set of shortest paths between any pair of vertices. This is a simple sparsifier that can be used for shortest path-finding problems and can be obtained via removing trivial edges <span class="math inline">\(w(u,v)\)</span> such that the shortest distance between <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span> is smaller than <span class="math inline">\(w(u,v)\)</span>.</p>
<div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> networkx <span class="im">as</span> nx</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># setup the graph</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>G <span class="op">=</span> nx.Graph()</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>G.add_nodes_from([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>])</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>G.add_edges_from([</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>  (<span class="dv">1</span>, <span class="dv">2</span>, {<span class="st">'w'</span>:<span class="dv">10</span>}),</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>  (<span class="dv">1</span>, <span class="dv">3</span>, {<span class="st">'w'</span>:<span class="dv">5</span>}),</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>  (<span class="dv">1</span>, <span class="dv">4</span>, {<span class="st">'w'</span>:<span class="dv">6</span>}), </span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>  (<span class="dv">2</span>, <span class="dv">3</span>, {<span class="st">'w'</span>:<span class="dv">3</span>}), </span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>  (<span class="dv">2</span>, <span class="dv">4</span>, {<span class="st">'w'</span>:<span class="dv">2</span>}), </span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>  (<span class="dv">3</span>, <span class="dv">4</span>, {<span class="st">'w'</span>:<span class="dv">6</span>})</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># setup plotting position of all vertices</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>pos<span class="op">=</span>{</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>  <span class="dv">1</span>:(<span class="dv">0</span>,<span class="dv">0</span>),</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>  <span class="dv">2</span>:(<span class="fl">0.5</span>,<span class="dv">1</span>),</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>  <span class="dv">3</span>:(<span class="dv">1</span>, <span class="dv">0</span>),</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>  <span class="dv">4</span>:(<span class="fl">0.5</span>, <span class="fl">0.5</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="co"># a simple networkx plotting function</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_graph():</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>  nx.draw_networkx(G,pos)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>  labels <span class="op">=</span> nx.get_edge_attributes(G,<span class="st">'w'</span>)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>  nx.draw_networkx_edge_labels(G,pos,edge_labels<span class="op">=</span>labels)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>  plt.axis(<span class="st">'off'</span>)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>  plt.show()</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="co"># before:</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>plot_graph()</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="co"># find the shortest path between any pair of vertices</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>shortest_paths <span class="op">=</span> <span class="bu">dict</span>(nx.all_pairs_dijkstra_path(G, weight<span class="op">=</span><span class="st">'w'</span>))</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> v <span class="kw">in</span> shortest_paths:</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> u <span class="kw">in</span> shortest_paths[v]:</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>      <span class="co"># if the edge from v to u has weight greater than the shortest path</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>      <span class="co"># between v and u, then remove it</span></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> v <span class="op">!=</span> u <span class="kw">and</span> <span class="bu">len</span>(shortest_paths[v][u]) <span class="op">&gt;</span> <span class="dv">2</span>:</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># remove edge from v to u if it exists</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> G.has_edge(v, u):</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>          G.remove_edge(v, u)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a><span class="co"># after:</span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>plot_graph()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-shortest-path-sparsification" class="cell quarto-layout-panel" data-fig-show="true" data-fig-size="300" data-execution_count="1">
<figure class="figure">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell-output cell-output-display quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-shortest-path-sparsification-1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="index_files/figure-html/fig-shortest-path-sparsification-output-1.png" class="img-fluid figure-img" data-ref-parent="fig-shortest-path-sparsification" width="540"></p>
<p></p><figcaption class="figure-caption">(a) The graph <span class="math inline">\(G\)</span> that we intend to sparsify.</figcaption><p></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-shortest-path-sparsification-2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="index_files/figure-html/fig-shortest-path-sparsification-output-2.png" class="img-fluid figure-img" data-ref-parent="fig-shortest-path-sparsification" width="540"></p>
<p></p><figcaption class="figure-caption">(b) The graph <span class="math inline">\(H\)</span> that is obtained by removing trivial edges.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;1: A simple illustration of a sparsifier that can help with shortest path problems.</figcaption><p></p>
</figure>
</div>
</div>
<p>On the other hand, <span class="citation" data-cites="benczur1996approximating">(<a href="#ref-benczur1996approximating" role="doc-biblioref">Benczúr and Karger 1996</a>)</span> for example introduces the cut-sparsifiers which are a class of sparsifiers that have almost identical cut weights for any set <span class="math inline">\(S \subset V\)</span>. In this report, we cover spectral graph sparsifiers which are a certain class of sparsifiers that have a tight connection with expander graphs and can approximate the Laplacian of a graph with high accuracy. Because of the close connection between graph spectral connectivity and edge connectivity introduced by Cheeger <span class="citation" data-cites="cheeger1970lower">(<a href="#ref-cheeger1970lower" role="doc-biblioref">Cheeger 1970</a>)</span> spectral sparsifiers were introduced by <span class="citation" data-cites="spielman2004nearly">(<a href="#ref-spielman2004nearly" role="doc-biblioref">Spielman and Teng 2004</a>)</span> and <span class="citation" data-cites="spielman2011spectral">(<a href="#ref-spielman2011spectral" role="doc-biblioref">Spielman and Teng 2011</a>)</span>. Conventionally, these graphs are constructed using randomized algorithms where we pick a certain edge of an original graph with a probability. For example, if an edge is crucial to the connectivity of our graph, then it has high importance and should be picked with high probability. However, in this report, we will show that we can construct a sparsifier with a deterministic algorithm introduced in <span class="citation" data-cites="batson2009twice">(<a href="#ref-batson2009twice" role="doc-biblioref">Batson, Spielman, and Srivastava 2009</a>)</span> that has a tight connection with the Ramanujan bounds.</p>
<p>Furthermore, we will cover an important reduction from the graph sparsification problem to a matrix approximation problem which has been further exploder in many follow-up papers <span class="citation" data-cites="tat2015constructing">(<a href="#ref-tat2015constructing" role="doc-biblioref">Tat Lee and Sun 2015</a>)</span> and <span class="citation" data-cites="lee2017sdp">(<a href="#ref-lee2017sdp" role="doc-biblioref">Lee and Sun 2017</a>)</span>. Moreover, this will give us the first deterministic algorithm for obtaining sparsifiers with linear edge count. That said, we have implemented the algorithm in Python and have tested it on a few graphs for illustration purposes.</p>
<p>Finally, we will focus our attention on running the algorithm on complete graphs. The sparsifier obtained from the complete graph will have high connectivity which resembles similarities with the expander graphs. Although the graph obtained from the algorithm is not regular, we will show that it has a lot of expander-like properties and we will draw a close connection with Ramanujan graphs.</p>
<section id="recap-and-preliminaries" class="level2">
<h2 class="anchored" data-anchor-id="recap-and-preliminaries">Recap and Preliminaries</h2>
<p>Here we will cover some preliminaries on spectral sparsification and then we will discuss the effective resistance-based algorithm for spectral sparsification. We will also discuss an important reduction to the matrix problem that was first formalized in <span class="citation" data-cites="batson2009twice">(<a href="#ref-batson2009twice" role="doc-biblioref">Batson, Spielman, and Srivastava 2009</a>)</span> which lays the groundwork for the final algorithm.</p>
<section id="spectral-sparsification" class="level3">
<h3 class="anchored" data-anchor-id="spectral-sparsification">Spectral Sparsification</h3>
<p>Before everything, we should define what a spectral sparsifier is. A spectral sparsifier is a sparse graph that approximates the Laplacian of a graph with high accuracy. In other words, a sparsifier is a graph that has a lot of the same properties as the original graph.</p>
<div id="def-spectral-sparsification" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1 </strong></span>A <span class="math inline">\((k, \epsilon)\)</span>-spectral sparsifier of a graph <span class="math inline">\(G = (V, E, w)\)</span> is a graph <span class="math inline">\(H\)</span> with <span class="math inline">\(k\)</span> edges such that, <span class="math display">\[L_G \approx_\epsilon L_H : (1 - \epsilon) L_G \preceq L_H \preceq (1 + \epsilon) L_G\]</span> where <span class="math inline">\(L_G\)</span> is the Laplacian of <span class="math inline">\(G\)</span> and <span class="math inline">\(L_H\)</span> is the Laplacian of <span class="math inline">\(H\)</span>.</p>
</div>
</section>
<section id="reduction-to-the-matrix-problem" class="level3">
<h3 class="anchored" data-anchor-id="reduction-to-the-matrix-problem">Reduction to the Matrix Problem</h3>
<p>Here, we will present an analog problem for the sparsification of matrices that is tightly connected to the spectral sparsification problem. The problem is as follows:</p>
<div id="def-matrix-approximation" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2 </strong></span><strong><span class="math inline">\((k, \epsilon)\)</span>-approximation of matrices</strong> Given a set of <span class="math inline">\(m\)</span> vectors <span class="math inline">\(v_1, \ldots, v_m \in \mathbb{R}^n\)</span> if <span class="math inline">\(A = \sum_{i=1}^m v_iv_i^T\)</span> is a positive semi-definite matrix, then we intend to find a subset of vectors <span class="math inline">\(\mathcal{S} \subseteq \{1, \ldots, m\}\)</span> of size <span class="math inline">\(k\)</span> and a set of coefficients <span class="math inline">\(s_i \in \mathbb{R}^+\)</span> such that <span class="math inline">\(\hat{A} = \sum_{i \in \mathcal{S}} s_i v_i v_i^T \approx_\epsilon A\)</span>.</p>
</div>
<p>Now we will show that one can solve the <span class="math inline">\((k, \epsilon)\)</span> problem in <a href="#def-matrix-approximation">Definition&nbsp;2</a> then it can plug into the graph sparsification problem and obtain a <span class="math inline">\((k, \epsilon)\)</span>-spectral sparsifier. To do so, observe that if we set <span class="math inline">\(A = L_G\)</span> and <span class="math inline">\(v_{ab} = \sqrt{w_G(a,b)} (\chi_a - \chi_b)\)</span> and <span class="math inline">\(s_{ab} = \frac{w_H(a,b)}{w_G(a,b)}\)</span>, then the problem in <a href="#def-matrix-approximation">Definition&nbsp;2</a> is equivalent to the spectral sparsification problem:</p>
<p><span class="math display">\[\begin{align*}
A = L_G &amp;= \sum_{(a,b) \in E(G)} w_G(a,b) L_{ab} \\
&amp;= \sum_{(a,b) \in E(G)} \sqrt{w_G(a,b)}^2 (\chi_a - \chi_b) (\chi_a - \chi_b)^T\\
&amp; = \sum_{ab \in E(G)} v_{ab} v_{ab}^T\\
\hat{A} = L_H &amp;= \sum_{(a, b) \in E(H)} w_H(a,b) L_{ab} \\
&amp;= \sum_{(a, b) \in E(H)} \frac{w_H(a,b)}{w_G(a,b)} \sqrt{w_G(a,b)}^2 (\chi_a - \chi_b) (\chi_a - \chi_b)^T\\
&amp;= \sum_{(a,b) \in E(H)} s_{ab} v_{ab} v_{ab}^T
\end{align*}\]</span></p>
</section>
<section id="sampling-based-sparsification" class="level3">
<h3 class="anchored" data-anchor-id="sampling-based-sparsification">Sampling-based Sparsification</h3>
<p>As alluded to previously, the problem of spectral sparsification can be approached from an edge-sampling perspective. In particular, one can assign importance weights to each edge and then come up with a sampling scheme that samples edges according to their importance. For example, an edge that is crucial for the connectivity of the graph has high importance for cut-sparsifiers or spectral sparsifiers. To that end, a set of edges can be independently sampled according to this scheme and after sampling each edge the graph becomes more and more similar to the original graph. However, since this sampling is done according to the measure of importance, even after sampling a small number of edges, the graph becomes a good approximation of the original graph.</p>
<p>One can also formulate the same thing for the matrix approximation problem. Assume that for each vector <span class="math inline">\(i\)</span>, we have a corresponding matrix <span class="math inline">\(X_i = s_i v_i v_i^T\)</span> which will be picked with probability <span class="math inline">\(p_i\)</span> and we will consider <span class="math inline">\(\hat{A} = \sum_{i \in \mathcal{S}} X_i\)</span> where <span class="math inline">\(\mathcal{S}\)</span> is the set of indices of the sampled vectors. One can bound the number of sampled vectors by coming up with good probabilities <span class="math inline">\(p_i\)</span> such that <span class="math inline">\(\sum p_i\)</span> is bounded by <span class="math inline">\(k\)</span> and can also bound the error of the approximation by using matrix concentration bounds. However, these algorithms tend to have the following problems:</p>
<ol type="1">
<li>The algorithm is not deterministic meaning that there is a very low chance of producing a large set <span class="math inline">\(\mathcal{S}\)</span>.</li>
<li>The algorithm is not deterministic meaning that there is a very low chance of producing an approximate <span class="math inline">\(\hat{A}\)</span> which is not close to <span class="math inline">\(A\)</span>.</li>
<li>Because these algorithms rely on exponential concentration bounds, typically they require to sample <span class="math inline">\(\mathcal{O}(n \cdot polylog(n))\)</span> vectors to achieve a good approximation.</li>
</ol>
<p>Although flawed, these solutions are easy and using this idea, a set of sampling techniques have been proposed to tackle the problem of sparsification with the most famous among them being the <strong>effective-resistance</strong> based sparsifiers <span class="citation" data-cites="spielman2008graph">(<a href="#ref-spielman2008graph" role="doc-biblioref">Spielman and Srivastava 2008</a>)</span>. We will briefly cover the main idea and intuition behind this and redirect the reader to other resources for further detailed reading.</p>
<p>The effective resistance between two nodes <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> is the equivalent resistance if we assume that the rest of the nodes are harmonic and only one external current is given to <span class="math inline">\(a\)</span> and one external current is taken from <span class="math inline">\(b\)</span>; then, the measured voltage difference between these two nodes will denote the effective resistance which can be written as <span class="math inline">\((\chi_a - \chi_b)^T L^+_G (\chi_a - \chi_b)\)</span> using Laplacians. Moreover, effective resistances have a combinatorial interpretation as well. If we assume we sample spanning trees proportional to their weight products, then the effective resistance between two nodes is proportional to the probability of the edge between those two nodes appearing. This means that a crucial edge in the connectivity, will have a high probability of appearing in the sampled spanning trees and thus will have a high effective resistance; that said, this will yield a high importance weight for that edge and thus it will be sampled more often:</p>
<p><strong>Effective-resistance based sparsifier</strong> For each edge <span class="math inline">\((a, b) \in E\)</span>, sample <span class="math inline">\((a,b)\)</span> with probability <span class="math inline">\(p(a,b) = \min\left(1, C\cdot (\log n) \epsilon^{-2} w(a,b) R_{eff}(a, b)\right)\)</span>. Where <span class="math inline">\(R_{eff}(a, b)\)</span> is the effective resistance between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. Using Rudelson concentration lemma <span class="citation" data-cites="rudelson1999random">(<a href="#ref-rudelson1999random" role="doc-biblioref">Rudelson 1999</a>)</span>, <span class="citation" data-cites="spielman2008graph">(<a href="#ref-spielman2008graph" role="doc-biblioref">Spielman and Srivastava 2008</a>)</span> shows that for a certain constant <span class="math inline">\(C\)</span> after picking <span class="math inline">\(\mathcal{O}(n\log n /\epsilon)\)</span> edges the resulting graph is a <span class="math inline">\(\epsilon\)</span>-spectral sparsifier with high probability.</p>
<!-- To show that, we use the following concentration-bound theorem:

::: {#thm-rudelson}
Let $X_1, \ldots, X_n$ be independent random positive semidefinite matrices such that $||X_i|| \le R$ almost surely. Let $X = \sum X_i$ and let $\mu_{\min}$ and $\mu_{\max}$ be the minimum and maximum eigenvalues of $E[X]$. Then, for $1 > \epsilon > 0$,
$$Pr\left[ \lambda_{\min} (\sum X_i) \le (1 - \epsilon) \mu_{\min} \right] \le n \left( \frac{e^{-\epsilon}}{(1 - \epsilon)^{(1 - \epsilon)}} \right)^{\mu_{\min}/R} \le e^{-\epsilon^2 / 2},$$
and for $\epsilon > 0$,
$$Pr\left[ \lambda_{\max} (\sum X_i) \le (1 + \epsilon) \mu_{\max} \right] \le n \left( \frac{e^{-\epsilon}}{(1 + \epsilon)^{(1 + \epsilon)}} \right)^{\mu_{\max}/R} \le e^{-\epsilon^2 / 3}.$$
:::

Now if we return to the idempotent matrix $\Pi$ in the previous section and plug in the Laplacians in the formula we obtain $\Pi = L_G^{+/2} L_G L_G^{+/2}$. Then, approximating $\Pi$ using $\hat{\Pi}$ will yield a graph that is a spectral sparsifier.
Now the idea is to assign a random positive semi-definite matrix to each edge $(a, b)$ and pick that matrix with a probability $p(a,b)$ which will give us a set of random matrices $X_{ab}$ that sum up to $\hat{\Pi}$. By applying a simple union bound we can show that:
$$Pr[\hat{\Pi} \approx_\epsilon \Pi] \ge 1 - Pr[\hat{\Pi} \succ (1 + \epsilon) \Pi]- Pr[\hat{\Pi} \prec (1 - \epsilon) \Pi]$${#eq-randomized-sparsification}
Hence, since $\Pi$ has only eigenvalues equal to one or zero, then we can apply @thm-rudelson with $\mu_{\min} = 1$ and $\mu_{\max} = 1$ because we only consider the space orthogonal to the kernel. Then we can show that both the values in the right-hand side of @eq-randomized-sparsification are at most $n 
\cdot e^{-\epsilon^2 / 3R}$ if the lemma conditions hold. We will indeed show that for $X_{ab} = w(a,b) / p(a,b) \cdot L_G^{+/2} L_{ab} L_G^{+/2}$, this is the case:

1. $\forall a, b \in V: ||X_{ab}|| \le R$

    Note that for $R = \frac{1}{C \log n \epsilon^{-2}}$ we can prove this:
    \begin{align*}
    ||X_{ab}|| &\le Tr(X_{ab}) = w(a,b) / p(a,b) \cdot Tr(L_G^{+/2} L_{ab} L_G^{+/2}) \\ 
    &\le w(a,b) / p(a,b) \cdot Tr(L_G^{+/2} (\chi_a - \chi_b) (\chi_a - \chi_b)^T L_G^{+/2}) \\ 
    & = w(a,b) / p(a,b) \cdot Tr((\chi_a - \chi_b)^T L_G^+ (\chi_a - \chi_b)) \\
    & = w(a,b) / p(a,b) \cdot R_{eff}(a, b) \le \frac{1}{C (\log n) \epsilon^{-2}}\\
    \end{align*}
2. $E[\sum_{a,b} X_{ab}] = \Pi$


    This can be easily shown by plugging in the formula:
    \begin{align*}
    E[\sum_{a,b} X_{ab}] &= \sum_{a,b} E[X_{ab}] = \sum_{a,b} p(a,b) \frac{w(a,b)}{p(a,b)} \cdot L_G^{+/2} L_{ab} L_G^{+/2}\\
    & = L_G^{+/2} \left( \sum_{a,b} w(a,b) L_{ab} \right) L_G^{+/2} = L_G^{+/2} L_{G} L_G^{+/2} = \Pi
    \end{align*}




::: {#cor-randomized-sparsification}
::: -->
</section>
</section>
<section id="method" class="level2">
<h2 class="anchored" data-anchor-id="method">Method</h2>
<p>We will now discuss the deterministic algorithm for approximating the matrix <span class="math inline">\(A\)</span>. The algorithm takes an iterative approach and follows <span class="math inline">\(k\)</span> iterations. At each iteration, it will pick a vector <span class="math inline">\(v_i\)</span> which corresponds to an edge and will add <span class="math inline">\(s_i v_i v_i^T\)</span> to the current accumulated matrix. After <span class="math inline">\(k\)</span> iterations it will give a good approximate for the matrix <span class="math inline">\(A\)</span>. But before we present the bulk of the algorithm, let’s start by laying some groundwork by presenting some useful intuitions.</p>
<section id="geometric-interpretation" class="level3">
<h3 class="anchored" data-anchor-id="geometric-interpretation">Geometric interpretation</h3>
<p>Note that for any pair of matrices <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, having the same null-space we have that <span class="math inline">\(A \succeq B \Longleftrightarrow I \succeq A^{+/2} B A^{+/2}\)</span>. Hence, <span class="math display">\[(1 - \epsilon) A \approx_\epsilon B \Longleftrightarrow \Pi \approx_\epsilon A^{+/2} B A^{+/2}\]</span> where <span class="math inline">\(\Pi = A^{+/2} A A^{+/2}\)</span> is the identity in the subspace orthogonal to the null space of <span class="math inline">\(A\)</span> and is an <em>idempotent</em> matrix. In other words, <span class="math inline">\(\Pi^2 = \Pi\)</span>. Therefore, without loss of generality, we may assume that <span class="math inline">\(A\)</span> in <a href="#def-matrix-approximation">Definition&nbsp;2</a> is an idempotent matrix <span class="math inline">\(\Pi\)</span> via the transformation described where <span class="math inline">\(A\)</span> is replaced by <span class="math inline">\(A^{+/2} A A^{+/2}\)</span> and <span class="math inline">\(v_i = A^{+/2} v_i\)</span> for all <span class="math inline">\(1 \le i \le m\)</span>.</p>
<p>With that in mind, thinking about idempotent matrices yields nice intuitions on how to think about the problem geometrically. Furthermore, for any positive semi-definite matrix <span class="math inline">\(M\)</span> we can define an ellipsoid <span class="math inline">\(\{x | x^T M x = 1\}\)</span> and for <span class="math inline">\(M = \Pi\)</span> being an idempotent matrix the ellipsoid corresponds to the sphere in the linearly transformed subspace of <span class="math inline">\(\Pi\)</span>: <span class="math display">\[x^T \Pi x = x^T \Pi \Pi x = ||\Pi x||_2^2 = 1.\]</span></p>
<p>Therefore, if we consider everything in the mapped subspace, i.e., replacing every vector <span class="math inline">\(x\)</span> with <span class="math inline">\(\Pi x\)</span> automatically, then we want to find a linear combination of their cross product such that the ellipsoid corresponding to that combination approximates a regular spherical shape. In other words, <span class="math display">\[\begin{align*}
&amp;\hat{A} = \sum s_i v_i v_i^T \approx_\epsilon A \\
\Longleftrightarrow &amp; ~ \hat{\Pi} = \sum s_i (A^{+/2}) v_i (A^{+/2} v_i)^T  \approx_\epsilon A^{+/2} A A^{+/2} = \Pi\\
\Longleftrightarrow &amp; ~ (1 - \epsilon) \Pi \preceq \hat{\Pi} \preceq (1 + \epsilon) \Pi \\
\Longleftrightarrow &amp; ~ \forall x : (1 - \epsilon) ||\Pi x||_2^2 \le [\Pi x]^T \hat{\Pi} [\Pi x] \le (1 + \epsilon) ||\Pi x||_2^2 \\
\end{align*}\]</span></p>
<p>Therefore, the ellipsoid projected using <span class="math inline">\(\Pi\)</span> is sandwiched between two spheres off by <span class="math inline">\(\epsilon\)</span> in their radius. Therefore, the algorithm takes an iterative approach to solve this geometric problem. It first starts of with <span class="math inline">\(\hat{A}^{(0)} = \emptyset\)</span> and then iteratively picks a vector <span class="math inline">\(v_i\)</span> and assigns a weight <span class="math inline">\(s_i\)</span> to it such that the ellipsoid <span class="math inline">\(\hat{A}^{(i)} = \hat{A}^{(i+1)} + s_i v_i v_i^T\)</span> becomes iteratively more like a sphere. To formalize this, the algorithm always bounds the corresponding ellipsoid between two spheres of radius <span class="math inline">\(l^{(i)}\)</span> and <span class="math inline">\(u^{(i)}\)</span>. At the beginning of each iteration, the lower bound <span class="math inline">\(l^{(i)}\)</span> will be increased by some <span class="math inline">\(\delta_l\)</span> and the lower bound <span class="math inline">\(u^{(i)}\)</span> will be increased by some <span class="math inline">\(\delta_u\)</span> and the algorithm will try to find a vector <span class="math inline">\(v_i\)</span> and a weight <span class="math inline">\(s_i\)</span> such that the new ellipsoid <span class="math inline">\(\hat{A}^{(i+1)}\)</span>. Moreover, the key idea here is to cleverly pick <span class="math inline">\(\delta_l\)</span> and <span class="math inline">\(\delta_u\)</span> values such that after <span class="math inline">\(k\)</span> iterations the gap between the two spheres is off by <span class="math inline">\(\epsilon\)</span>. In other words, the following should hold: <span class="math display">\[\frac{u^{(0)} + k \delta_u}{l^{(k)} + k \delta_l} = \frac{u^{(k)}}{l^{(k)}} \le \frac{1 + \epsilon}{1 - \epsilon}.\]</span> This will ensure that the shape of the ellipsoid becomes more and more spherical as the algorithm progresses, and finally, a simple scaling will yield an approximate unit sphere which is what we want.</p>
<p>For illustration, the following shows the algorithm in action. The algorithm starts with an initial ellipsoid that is not spherical and then iteratively picks a vector <span class="math inline">\(v_i\)</span> and a weight <span class="math inline">\(s_i\)</span> such that it still remains sandwiched between two spheres of radius <span class="math inline">\(l^{(i)}\)</span> and <span class="math inline">\(u^{(i)}\)</span>. Note that in this example <span class="math inline">\(\delta_l\)</span> and <span class="math inline">\(\delta_u\)</span> are equal, therefore, for a large enough <span class="math inline">\(k\)</span> the ellipsoid will become spherical because although the radius is growing the gap remains the same, further limiting the range of the ellipsoid.</p>
<p><img src="figs/algorithm.gif" class="img-fluid" alt="The schematic of the algorithm in action."> <!-- ```{python}
#| label: fig-ellipsoid
#| fig-cap: "A dummy algorithm that approximates a unit sphere"
#| fig-alt: "A dummy algorithm that approximates a unit sphere"
#| fig-size: 400

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Ellipse

def plot_ellipse(ax, A, color, alpha):
  w, v = np.linalg.eig(A)
  w = np.sqrt(w)
  ell = Ellipse(
    xy = (0, 0), 
    width = 2 * w[0], 
    height = 2 * w[1], 
    angle = np.rad2deg(np.arccos(v[0, 0])), 
    color = color, 
    alpha = alpha
  )
  ax.add_artist(ell)
  ax.set_xlim(-10, 10)
  ax.set_ylim(-10, 10)
  ax.set_aspect('equal')

chiz = [[[10, 1], [1, 10]], [[3, -1], [-1, 3]], [[5, 2], [2, 3]]]
for i, chi in enumerate(chiz):
  plt.figure(i)
  plot_ellipse(ax, np.array(chi), 'blue', 0.2)
  plt.show()
``` --></p>
</section>
</section>
<section id="physical-view-and-the-expected-behavior" class="level2">
<h2 class="anchored" data-anchor-id="physical-view-and-the-expected-behavior">Physical View and the Expected behavior</h2>
<p>The fact that <span class="math inline">\(\hat{A}^{(i)}\)</span> should be bounded between two spheres translates into all the eigenvalues of <span class="math inline">\(\hat{A}^{(i)}\)</span> being bounded between the two radiuses. Therefore, an important observation is to monitor what happens to the eigenvalues of <span class="math inline">\(\hat{A}^{(i)}\)</span> when <span class="math inline">\(vv^T\)</span> is being added at each iteration. To do so, we consider the characteristic polynomial of <span class="math inline">\(A\)</span> at each iteration written as <span class="math inline">\(p_A(\lambda) = \det(\lambda I - A)\)</span>. There are two important lemmas when analyzing <span class="math inline">\(A + vv^T\)</span> matrices, one is the Sherman-Morrison lemma which states that:</p>
<div id="lemma-sherman-morrison">
<p>Suppose <span class="math inline">\(A\)</span> is an invertible square matrix and <span class="math inline">\(u, v\)</span> are column vectors. Then <span class="math inline">\(A + uv^T\)</span> is invertible iff <span class="math inline">\(1 + v^T A^{-1} u \neq 0\)</span>. In this case, <span class="math display">\[(A + uv^T)^{-1} = A^{-1} - \frac{A^{-1}uv^TA^{-1}}{1 + v^TA^{-1}u}\]</span></p>
</div>
<p>The other is the matrix determinant lemma which states that:</p>
<div id="lemma-matrix-determinant">
<p>Suppose <span class="math inline">\(A\)</span> is an invertible square matrix and <span class="math inline">\(u, v\)</span> are column vectors. Then <span class="math display">\[\det(A + uv^T) = \det(A) (1 + v^T A^{-1} u)\]</span></p>
</div>
<p>Moreover, plugging these into the characteristic polynomial of <span class="math inline">\(A + vv^T\)</span> yields the following:</p>
<p><span class="math display">\[
\begin{align*}
p_{A + vv^T}(\lambda) &amp;= \det(\lambda I - A - vv^T) \\
&amp; = \det(\lambda I - A) (1 - v^T \left(\lambda I - A \right)^{-1}u) \\
&amp; = \det (\lambda I - A) \left(1 - v^T \left[\sum_{i=1}^n \frac{1}{\lambda - \lambda_i} u_i u_i^T\right] v\right)\\
&amp; = p_A(\lambda) \left(1 -  \sum_{i=1}^n \frac{(v^Tu_i)^2}{\lambda - \lambda_i}\right)\\
\end{align*}
\]</span></p>
<p>Therefore, we can assume particles being set on each of the <span class="math inline">\(\lambda_i\)</span> values with the <span class="math inline">\(i\)</span>th one on <span class="math inline">\(\lambda_i\)</span> with charge <span class="math inline">\(v^Tu_i\)</span>. The new set of equilibrium points for this particle set will entail the new eigenvalues of <span class="math inline">\(A + vv^T\)</span> which are the roots of <span class="math inline">\(p_{A + vv^T}(\lambda)\)</span>. Note that for <span class="math inline">\(u_i\)</span> values such that <span class="math inline">\(v^Tu_i=0\)</span> the charge is zero and therefore, the new eigenvalues will be the same as the old ones.</p>
<p>The following figure illustrates the matrix case <span class="math inline">\(A\)</span> with three different vectors <span class="math inline">\(v_1\)</span>, <span class="math inline">\(v_2\)</span> and <span class="math inline">\(v_3\)</span>. Each color corresponds to the characteristic polynomial for different <span class="math inline">\(v\)</span> values where,</p>
<p><span class="math display">\[A = \lambda_1 u_1 u_1^T + \lambda_2 u_2 u_2^T + \lambda_3 u_3 u_3^T
= \begin{bmatrix}
1.6 &amp; -0.2 &amp; -0.33\\
-0.2 &amp; 3.4 &amp; -0.33\\
-0.33 &amp; -0.33 &amp; 1
\end{bmatrix}\]</span> <span class="math display">\[\begin{bmatrix}\lambda_1 \\ \lambda_2 \\ \lambda_3\end{bmatrix} = \begin{bmatrix}0.79 \\ 1.75 \\ 3.46\end{bmatrix}, u_1 = \begin{bmatrix}
-0.41\\
-0.15\\
-0.9
\end{bmatrix}, u_2 = \begin{bmatrix}
-0.9 \\
-0.03 \\
0.42
\end{bmatrix}, u_3 = \begin{bmatrix}
0.08\\
-0.99\\
0.12\\
\end{bmatrix}\]</span> We note that <span class="math inline">\(\langle v_i, u_j \rangle^2\)</span> is the charge of particle <span class="math inline">\(j\)</span> when adding <span class="math inline">\(v_i\)</span> to <span class="math inline">\(A\)</span> and we can summarize all the charged particles in the following matrix: <span class="math display">\[v_1 = \begin{bmatrix}
0\\
1\\
1\\
\end{bmatrix}, v_2 = \begin{bmatrix}
1\\
1\\
0\\
\end{bmatrix},
C = \begin{bmatrix}
1.10 &amp; 0.15 &amp; 0.75\\
0.31 &amp; 0.87 &amp; 0.82
\end{bmatrix}, C_{ij} = \langle v_i, u_j \rangle^2\]</span></p>
<div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_characteristic_polynomial(A, v, color):</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>  x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">25</span>, <span class="dv">50</span>, <span class="dv">1000</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>  w, u <span class="op">=</span> np.linalg.eig(A)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># plot the determinant of xI - A</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>  y <span class="op">=</span> []</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>  roots <span class="op">=</span> []</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>  prv <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> i <span class="kw">in</span> x:</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    val <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> np.<span class="bu">sum</span>(<span class="dv">1</span><span class="op">/</span>(i <span class="op">-</span> w) <span class="op">*</span> (v <span class="op">@</span> u)<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> prv <span class="op">&lt;</span> <span class="dv">0</span> <span class="kw">and</span> val <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>      roots.append(i)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    prv <span class="op">=</span> val</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    y.append(val)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>  plt.plot(x, y, color <span class="op">=</span> color, label<span class="op">=</span><span class="st">'characteristic polynomial of A + vv^T'</span>)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>  plt.scatter(roots, np.zeros(<span class="bu">len</span>(roots)), color <span class="op">=</span> color, marker <span class="op">=</span> <span class="st">'o'</span>, label<span class="op">=</span><span class="st">'new-eigenvalues'</span>)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="co"># create an orthonormal 3 by 3 matrix U</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>U <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">1</span>, <span class="op">-</span><span class="dv">2</span>], [<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>]])</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>U <span class="op">=</span> U <span class="op">/</span> np.linalg.norm(U, axis <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> U <span class="op">@</span> np.diag([<span class="dv">1</span> , <span class="dv">2</span>, <span class="dv">3</span>]) <span class="op">@</span> U.T</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>vz <span class="op">=</span> [[<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>]]</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> [<span class="st">'blue'</span>, <span class="st">'red'</span>]</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the eigenvalues of A</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>w, u <span class="op">=</span> np.linalg.eig(A)</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a><span class="co"># sort according to w and reorder u</span></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>w, u <span class="op">=</span> <span class="bu">zip</span>(<span class="op">*</span><span class="bu">sorted</span>(<span class="bu">zip</span>(w, u.T)))</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>u <span class="op">=</span> np.array(u).T</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> np.array(w)</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> col, v <span class="kw">in</span> <span class="bu">zip</span>(colors, vz):</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>  plt.scatter(w, np.zeros(w.shape), color <span class="op">=</span> <span class="st">'black'</span>, marker <span class="op">=</span> <span class="st">'x'</span>, label<span class="op">=</span><span class="st">'previous eigenvalues'</span>)</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>  <span class="co"># add text with textbox equal to np.sum(w) on top of each eigenvalue</span></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> i, wi <span class="kw">in</span> <span class="bu">enumerate</span>(w):</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>    t <span class="op">=</span> plt.text(wi <span class="op">-</span> <span class="fl">0.5</span>, <span class="fl">0.1</span> <span class="op">*</span> (<span class="dv">4</span> <span class="op">*</span> (i <span class="op">%</span> <span class="dv">2</span>) <span class="op">-</span> <span class="fl">2.5</span>), <span class="ss">f"c=</span><span class="sc">{</span>(v <span class="op">@</span> u[:,i])<span class="op">**</span><span class="dv">2</span><span class="sc">:.2f}</span><span class="ss">"</span>, color <span class="op">=</span> <span class="st">'black'</span>)</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>    t.set_bbox(<span class="bu">dict</span>(facecolor<span class="op">=</span><span class="st">'white'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, edgecolor<span class="op">=</span>col))</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>  plot_characteristic_polynomial(A, v, col)</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>  plt.xlim(<span class="dv">0</span>, <span class="fl">5.5</span>)</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>  plt.ylim(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>  plt.legend()</span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>  plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-matrix-determinant" class="cell quarto-layout-panel" data-execution_count="2">
<figure class="figure">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell-output cell-output-display quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-matrix-determinant-1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="index_files/figure-html/fig-matrix-determinant-output-1.png" class="img-fluid figure-img" data-ref-parent="fig-matrix-determinant" width="582"></p>
<p></p><figcaption class="figure-caption">(a) The characteristic polynomial after adding <span class="math inline">\(v_1\)</span> to A.</figcaption><p></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-matrix-determinant-2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="index_files/figure-html/fig-matrix-determinant-output-2.png" class="img-fluid figure-img" data-ref-parent="fig-matrix-determinant" width="582"></p>
<p></p><figcaption class="figure-caption">(b) The characteristic polynomial after adding <span class="math inline">\(v_2\)</span> to A.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;2: The characteristic polynomial of <span class="math inline">\(A + vv^T\)</span> for different <span class="math inline">\(v\)</span> values, the higher the charge the more it will repel the new eigenvalues from the old ones.</figcaption><p></p>
</figure>
</div>
</div>
<p>The goal is to pick a <span class="math inline">\(v\)</span> such that all the eigenvalues are uniformly pushed forward so that they can stay between the new ranges <span class="math inline">\(l^{(i+1)}\)</span> and <span class="math inline">\(u^{(i+1)}\)</span>. To get a sense, let’s pick one of the <span class="math inline">\(m\)</span> vectors with uniform probability and add it to <span class="math inline">\(A\)</span>. In that case, the expected charges can be written as: <span class="math display">\[E[\langle v, u_j \rangle^2] = \frac{1}{m} \sum_{i=1}^m \langle v_i, u_j \rangle^2 = \frac{1}{m} u_j^T \left( \sum_{i=1}^m v_i v_i^T \right)u_j = \frac{||\Pi u_j||_2^2}{m} = \frac{1}{m}\]</span> Hence, on expectation all the particles have charge <span class="math inline">\(1/m\)</span> and the expected deterministic polynomial is: <span class="math display">\[
\begin{align*}
E[p_{A + v}(\lambda)] &amp;= p_A(\lambda) E\left[1 - \sum_{i=1}^m \frac{\langle u_i, v\rangle^2}{\lambda - \lambda_i}\right] = p_A(\lambda) \left(1 - \sum_{i=1}^m \frac{E\langle u_i, v\rangle^2}{\lambda - \lambda_i}\right)\\
&amp; = p_A(\lambda) \left(1 - \sum_{i=1}^m \frac{1/m}{\lambda - \lambda_i}\right) = p_A(\lambda) - \frac{1}{m} \sum_{i=1}^m \frac{p_A(\lambda)}{\lambda - \lambda_i}\\
&amp; = p_A(\lambda) - \frac{1}{m} \sum_{i=1}^m \prod_{1 = j\neq i}^m (\lambda - \lambda_j)\\
&amp;= p_A(\lambda) - \frac{1}{m} p'_A(\lambda)\\
\end{align*}
\]</span> Therefore, if we start off with the matrix <span class="math inline">\(p_{A^{(0)}}(\lambda) = \lambda^n\)</span>, after <span class="math inline">\(nd\)</span> iterations the expected polynomial is a set of associate Laguerre polynomials that are well studied <span class="citation" data-cites="dette1995some">(<a href="#ref-dette1995some" role="doc-biblioref">Dette and Studden 1995</a>)</span>, and in particular, it has been proven that the ratio between the largest and smallest root for these polynomials is bounded by the value below:</p>
<p><span class="math display">\[\frac{d + 1 + 2\sqrt{d}}{d + 1 - 2\sqrt{d}} \xrightarrow{\epsilon = \frac{2\sqrt{d}}{d+1}} \frac{1 + \epsilon
}{1 - \epsilon}\]</span></p>
<p>Although this is just speculation and no <span class="math inline">\(v_i\)</span> values will necessarily exist with the expected behavior, we can still get an idea of the goal <span class="math inline">\(\epsilon\)</span> and come up with the following proposition:</p>
<div id="prp-final-form" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 1 </strong></span>For any matrix <span class="math inline">\(A = \sum_{i=1}^m v_i v_i^T\)</span> we can choose a subset <span class="math inline">\(\mathcal{S}\)</span> of <span class="math inline">\(v_i\)</span> and a set of coefficients <span class="math inline">\(s_i\)</span> with size <span class="math inline">\(nd\)</span> such that: <span class="math display">\[\hat{A} = \sum_{i \in \mathcal{S}} s_i \cdot v_i v_i^T,~~ (1 - \frac{2\sqrt{d}}{d+1}) A \preceq \hat{A} \preceq (1 + \frac{2\sqrt{d}}{d+1}) A\]</span></p>
</div>
<p>The graph formulation of <a href="#prp-final-form">Proposition&nbsp;1</a> is as follows:</p>
<div id="cor-final-form" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 1 </strong></span>For any graph <span class="math inline">\(G\)</span> and any <span class="math inline">\(\epsilon\)</span> we can choose a subset of <span class="math inline">\(\mathcal{O}(n/\epsilon^2)\)</span> edges with arbitrary edge weights to obtain <span class="math inline">\(H\)</span> such that <span class="math inline">\(H\)</span> is an <span class="math inline">\(\epsilon\)</span>-sparsifier of <span class="math inline">\(G\)</span>: <span class="math inline">\(L_G \approx_\epsilon L_H\)</span>.</p>
</div>
<p>This is set using <span class="math inline">\(\epsilon = \frac{2\sqrt{d}}{d + 1}\)</span> where <span class="math inline">\(\frac{n}{\epsilon^2} = \mathcal{O}(nd)\)</span>. In the next section, we will see how we can choose <span class="math inline">\(v_i\)</span> and <span class="math inline">\(s_i\)</span> at each step such that after <span class="math inline">\(nd\)</span> iterations this happens.</p>
<section id="potential-functions" class="level3">
<h3 class="anchored" data-anchor-id="potential-functions">Potential Functions</h3>
<p>The big question is, how can we quantize the boundedness of the matrix <span class="math inline">\(A\)</span> at each step? We want <span class="math inline">\(A^{(i)}\)</span> to have eigenvalues that are bounded by <span class="math inline">\(l^{(i+1)}\)</span> and <span class="math inline">\(u^{(i+1)}\)</span>; and so, we use a family of <strong>potential functions</strong> that explode when the eigenvalues approach the bounds. A set of such potentials can be chosen using the fact that <span class="math inline">\(uI - A\)</span> or <span class="math inline">\(A - lI\)</span> will have infinitely small eigenvalues when the eigenvalues of <span class="math inline">\(A\)</span> approach <span class="math inline">\(u\)</span> or <span class="math inline">\(l\)</span> respectively; therefore, their inverse will be ill-conditioned and have infinitely large eigenvalues. We can use the following potential functions:</p>
<p><span class="math display">\[\Phi_{u, l}(A) = \Phi_u(A) + \Phi_l(A) = Tr[(uI - A)^{-1}] + Tr[(A - l I)^{-1}]\]</span></p>
<p>In summary, the main idea is to choose <span class="math inline">\(v_i\)</span> and <span class="math inline">\(s_i\)</span> such that the potential for the matrix <span class="math inline">\(A^{(i)}\)</span> in the next iteration does not explode. To do so, we ensure that the potentials remain monotonically decreasing:</p>
<p><span class="math display">\[\infty \gg \Phi^{u^{(0)}}(A^{(0)}) \ge \Phi^{u^{(1)}}(A^{(1)}) \ge ... \ge \Phi^{u^{(nd)}}(A^{(nd)})\]</span> <span class="math display">\[\infty \gg \Phi^{\ell^{(0)}}(A^{(0)}) \ge \Phi^{\ell^{(1)}}(A^{(1)}) \ge ... \ge \Phi^{\ell^{(nd)}}(A^{(nd)})\]</span></p>
<p>With that in mind, let’s assume we are going to assign <span class="math inline">\(s_k\)</span> to any vector <span class="math inline">\(v_k\)</span> such that after the increase in our upper and lower bound, the potential remains non-increasing. Now let us separately consider the upper and lower bound potentials.</p>
<p>When increasing <span class="math inline">\(l^{(i)}\)</span>, the eigenvalues come closer to the lower bound, and hence, the potential of the lower bound will increase; therefore, for any vector <span class="math inline">\(v_k\)</span>, the coefficient <span class="math inline">\(s_k\)</span> should be bounded by some value <span class="math inline">\(L_{A^{(i)}}(v_k)\)</span> such that after adding <span class="math inline">\(s_k \cdot v_k v_k^T\)</span> to <span class="math inline">\(A^{(i)}\)</span>, spectrum shifts forward and the increase in the potential cancels out. That said, for any matrix <span class="math inline">\(A\)</span> and any vector <span class="math inline">\(v\)</span> we have: <span class="math display">\[
\begin{align*}
&amp;\Phi^{\overset{l'}{\overbrace{l + \delta_l}}}(A + s \cdot vv^T) \le \Phi^l(A)\\
\Phi^{l'}(A + s \cdot vv^T) &amp; = Tr(A + s \cdot vv^T - l'I)^{-1}  \qquad \text{Sherman-Morrison}\\\
&amp; = Tr\left((A - l'I)^{-1}\right) + Tr\left(\frac{s \cdot (A - l'I)^{-1} v v^T (A - l'I)^{-1}}{1 + s \cdot v^T (A - l' I)^{-1} v}\right)\\
&amp;= \Phi^{l'}(A) - \frac{s \cdot v^T (A - l'I)^{-2}v}{1 + s \cdot v^T  (A - l'I)^{-1}v} \le \Phi^l(A)\\
\Leftrightarrow &amp;~ \underset{\Delta}{\underbrace{\Phi^{l'}(A) - \Phi^l(A)}} \le \frac{s \cdot v^T (A - l'I)^{-2}v}{1 + s \cdot v^T  (A - l'I)^{-1}v}\\
\Leftrightarrow &amp;~ s\cdot \left[v^T (A - l'I)^{-2}v  - \Delta v^T(A - l' I)^{-1} v\right] \ge \Delta\\
\Leftrightarrow &amp;~ s \ge \frac{\Delta}{v^T \left( (A - l'I)^{-2} - \Delta (A - l' I)^{-1} \right) v} = L_A(v)\\
\end{align*}\]</span> {#eqn-l-potential}</p>
<p>On the other hand, a similar thing can be said for the upper-bound potential. when increasing <span class="math inline">\(u^{(i)}\)</span>, the eigenvalues are further away from the upper bound which gives us the freedom to shift the eigenvalues forward. However, this shifting should not be so extreme that the potential at most increases offset the decrease introduced after adding <span class="math inline">\(\delta_u\)</span> to the potential. <span class="math display">\[\Phi^{\overset{u'}{\overbrace{u + \delta_u}}}(A + s \cdot vv^T) \le \Phi^u(A)\]</span></p>
<p>Similar to <span class="citation" data-cites="eqn-l-potential">(<a href="#ref-eqn-l-potential" role="doc-biblioref"><strong>eqn-l-potential?</strong></a>)</span>, if we negate <span class="math inline">\(s\)</span> and <span class="math inline">\(A\)</span> then the upper-bound potential will act similarly to the lower-bound potential. Therefore, we can write the following: <span class="math display">\[s \le U_A(v) = \frac{\Delta}{v^T \left((u' I - A)^{-2} - \Delta (u' I - A)^{-1}\right)v}, ~~ \Delta = \Phi^{u}(A) - \Phi^{u'}(A)\]</span></p>
<p>Finally, for every vector <span class="math inline">\(v_i\)</span> at each step, we can introduce an upper and lower bound for the coefficient corresponding to that vector. However, this is not enough to ensure that at least one <span class="math inline">\(v_i\)</span> exists such that <span class="math inline">\(L_A\)</span>; in other words, it might be the case that for each vector the upper and lower bounds are contradictory which will put the algorithm in a stale-mate state. To avoid this, we pick the values <span class="math inline">\(\delta_u\)</span> and <span class="math inline">\(\delta_l\)</span> carefully and introduce a nice lemma in the next section that ensures always such a vector exists.</p>
</section>
<section id="the-existence-of-a-good-vector" class="level3">
<h3 class="anchored" data-anchor-id="the-existence-of-a-good-vector">The Existence of a good Vector</h3>
</section>
<section id="the-deterministic-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="the-deterministic-algorithm">The Deterministic Algorithm</h3>
<p>For a demonstration of a line plot on a polar axis, see <a href="#fig-polar">Figure&nbsp;3</a>.</p>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>r <span class="op">=</span> np.arange(<span class="dv">0</span>, <span class="dv">2</span>, <span class="fl">0.01</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>theta <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> np.pi <span class="op">*</span> r</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>  subplot_kw <span class="op">=</span> {<span class="st">'projection'</span>: <span class="st">'polar'</span>} </span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>ax.plot(theta, r)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>ax.set_rticks([<span class="fl">0.5</span>, <span class="dv">1</span>, <span class="fl">1.5</span>, <span class="dv">2</span>])</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-polar" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="index_files/figure-html/fig-polar-output-1.png" width="450" height="439" class="figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3: A line plot on a polar axis</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="experimental-details" class="level3">
<h3 class="anchored" data-anchor-id="experimental-details">Experimental Details</h3>
<p>– TODO: show the results of using the deterministic algorithm vs the randomized algorithm</p>
</section>
</section>
<section id="sparsification-of-complete-graphs" class="level2">
<h2 class="anchored" data-anchor-id="sparsification-of-complete-graphs">Sparsification of Complete Graphs</h2>
<section id="expander-graphs" class="level3">
<h3 class="anchored" data-anchor-id="expander-graphs">Expander Graphs</h3>
<p>– TODO: recap of expanders</p>
<p>– TODO: expander mixing lemma</p>
</section>
<section id="ramanujan-bounds" class="level3">
<h3 class="anchored" data-anchor-id="ramanujan-bounds">Ramanujan Bounds</h3>
</section>
<section id="the-twice-ramanujan-sparsifier" class="level3">
<h3 class="anchored" data-anchor-id="the-twice-ramanujan-sparsifier">The Twice Ramanujan Sparsifier</h3>
<p>– TODO: illustration of what happens in the algorithm iteratively on a complete graph.</p>
</section>
<section id="references" class="level3">
<h3 class="anchored" data-anchor-id="references">References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-batson2009twice" class="csl-entry" role="doc-biblioentry">
Batson, Joshua D, Daniel A Spielman, and Nikhil Srivastava. 2009. <span>“Twice-Ramanujan Sparsifiers.”</span> In <em>Proceedings of the Forty-First Annual ACM Symposium on Theory of Computing</em>, 255–62.
</div>
<div id="ref-benczur1996approximating" class="csl-entry" role="doc-biblioentry">
Benczúr, András A, and David R Karger. 1996. <span>“Approximating St Minimum Cuts in <span>Õ</span> (n 2) Time.”</span> In <em>Proceedings of the Twenty-Eighth Annual ACM Symposium on Theory of Computing</em>, 47–55.
</div>
<div id="ref-cheeger1970lower" class="csl-entry" role="doc-biblioentry">
Cheeger, Jeff. 1970. <span>“A Lower Bound for the Smallest Eigenvalue of the Laplacian, Problems in Analysis, a Symposium in Honor of s.”</span> <em>Bochner, Princeton U. Press, Princeton</em>.
</div>
<div id="ref-chew1989there" class="csl-entry" role="doc-biblioentry">
Chew, L Paul. 1989. <span>“There Are Planar Graphs Almost as Good as the Complete Graph.”</span> <em>Journal of Computer and System Sciences</em> 39 (2): 205–19.
</div>
<div id="ref-dette1995some" class="csl-entry" role="doc-biblioentry">
Dette, Holger, and William J Studden. 1995. <span>“Some New Asymptotic Properties for the Zeros of Jacobi, Laguerre, and Hermite Polynomials.”</span> <em>Constructive Approximation</em> 11 (2): 227–38.
</div>
<div id="ref-lee2017sdp" class="csl-entry" role="doc-biblioentry">
Lee, Yin Tat, and He Sun. 2017. <span>“An Sdp-Based Algorithm for Linear-Sized Spectral Sparsification.”</span> In <em>Proceedings of the 49th Annual Acm Sigact Symposium on Theory of Computing</em>, 678–87.
</div>
<div id="ref-rudelson1999random" class="csl-entry" role="doc-biblioentry">
Rudelson, Mark. 1999. <span>“Random Vectors in the Isotropic Position.”</span> <em>Journal of Functional Analysis</em> 164 (1): 60–72.
</div>
<div id="ref-spielman2008graph" class="csl-entry" role="doc-biblioentry">
Spielman, Daniel A, and Nikhil Srivastava. 2008. <span>“Graph Sparsification by Effective Resistances.”</span> In <em>Proceedings of the Fortieth Annual ACM Symposium on Theory of Computing</em>, 563–68.
</div>
<div id="ref-spielman2004nearly" class="csl-entry" role="doc-biblioentry">
Spielman, Daniel A, and Shang-Hua Teng. 2004. <span>“Nearly-Linear Time Algorithms for Graph Partitioning, Graph Sparsification, and Solving Linear Systems.”</span> In <em>Proceedings of the Thirty-Sixth Annual ACM Symposium on Theory of Computing</em>, 81–90.
</div>
<div id="ref-spielman2011spectral" class="csl-entry" role="doc-biblioentry">
———. 2011. <span>“Spectral Sparsification of Graphs.”</span> <em>SIAM Journal on Computing</em> 40 (4): 981–1025.
</div>
<div id="ref-tat2015constructing" class="csl-entry" role="doc-biblioentry">
Tat Lee, Yin, and He Sun. 2015. <span>“Constructing Linear-Sized Spectral Sparsification in Almost-Linear Time.”</span> <em>arXiv e-Prints</em>, arXiv–1508.
</div>
</div>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>